<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: shell | Code Ramblings]]></title>
  <link href="http://felix.oghina.com/categories/shell/atom.xml" rel="self"/>
  <link href="http://felix.oghina.com/"/>
  <updated>2014-07-23T22:22:25+03:00</updated>
  <id>http://felix.oghina.com/</id>
  <author>
    <name><![CDATA[Felix OghinÄƒ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Git: Moving History]]></title>
    <link href="http://felix.oghina.com/2014/07/23/git-moving-history/"/>
    <updated>2014-07-23T20:37:24+03:00</updated>
    <id>http://felix.oghina.com/2014/07/23/git-moving-history</id>
    <content type="html"><![CDATA[<p>The title of this post may make it sound like it&rsquo;s going to be about how git changed the face of software development and entered us into a
new era of coding collaboration. No. This post is about how to copy a subtree from one git repository to another, while keeping its history.</p>

<p>Let&rsquo;s say you&rsquo;ve been working on a component in an incubator-style repository, and it&rsquo;s time to move it to the main project&rsquo;s repository.
Simply copying the code would destroy the valuable history in the target repo, so that should be avoided. There are also lots of other
components in the incubator repo that you don&rsquo;t want to move.</p>

<h2>Step 1: Export your component to a temporary repository</h2>

<p>You can do this using <code>git filer-branch</code>. <em>Warning:</em> this will reduce your local copy of the incubator repository to <em>just</em> your component.
Either clone it locally to a different path or be prepared to clone it again from the remote.</p>

<p><code>console
incubator$ git filter-branch --subdirectory-filter components/my-component -- --all
</code></p>

<p>Now your repository has been reduced to just the contents <strong>and history</strong> of <code>components/my-component</code>.</p>

<h2>Step 2: Create a patch for the entire history of your component</h2>

<p>Use <code>git format-patch</code> to export all of your commits to a patch file. Props to <a href="https://twitter.com/rombert">@rombert</a> for this idea.</p>

<p><code>console
incubator$ git format-patch --stdout --root $(git rev-list HEAD | tail -n 1) HEAD &gt; my-component.patch
</code></p>

<p>I feel like this command could use some explaining. First off, we&rsquo;re telling <code>format-patch</code> to output everything to <code>--stdout</code>. Otherwise,
it would create one patch file per commit, which can get pretty clumsy if there are lots of them. Second, we&rsquo;re passing the output of
<code>git rev-list HEAD | tail -n 1</code> for the <code>--root</code> parameter. The enclosed command will find find the <code>sha1</code> of the very first commit, while
the <code>--root</code> parameter will tell <code>format-patch</code> to include that commit, not start from it. Lastly, <code>HEAD</code> is the target ref, which is
basically the most recent commit.</p>

<h2>Step 3: Apply the patch</h2>

<p>Now it&rsquo;s time to add your component to the main project. Of course, you don&rsquo;t want to add your component to the root of the repository
(which is what all the paths in the patch are relative to). Thankfully, git supports prepending a path to all filenames in a patch.</p>

<p><code>console
main-project$ git am --directory components/my-component my-component.patch
</code></p>

<p>Now your main project contains your component and all of its history. Some notes:</p>

<ol>
<li>Only one branch will be copied. If your component development happened on multiple branches, that will be lost.</li>
<li>No tags will be copied to the main repository, you will need to tag your commits manually.</li>
<li>Commit IDs will change, so you will not be able to <a href="https://github.com/foghina/git-copy-tags">copy your tags automatically</a>.</li>
</ol>


<p>Pretty neat.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Video Surveillance... in Bash]]></title>
    <link href="http://felix.oghina.com/2012/11/18/video-surveillance-in-bash/"/>
    <updated>2012-11-18T00:47:00+02:00</updated>
    <id>http://felix.oghina.com/2012/11/18/video-surveillance-in-bash</id>
    <content type="html"><![CDATA[<p>My parents recently asked me to install an IP video surveillance camera at their
house (for reasons). Since I already have a Linux server there running 24/7, it
was only natural that I somehow set it up to record the camera&rsquo;s video feed (mp4
over RTSP). After a bit of googling I found <a href="http://www.zoneminder.com/">ZoneMinder</a>, which looked like
it did everything I needed. However, after wasting half a day fiddling with it,
I realized that it is, although very complex and feature-rich, not up to my
needs: no matter how you configure it, it will convert the video feed into
thousands of JPEG files and store them randomly on the filesystem. It also uses
a MySQL database to store its information (so extra dependencies), viewing the
camera live is very slow (refreshes a static image every 4-5 seconds), replaying
a recording needs the Java plug-in in your browser (which I don&rsquo;t have on
Ubuntu) and you have to &ldquo;export&rdquo; a movie clip (i.e. it has to convert all the
images it saved back to a video). What a waste, but I can see how this solution
could be the most compatible with all possible environments (camera types, OSes,
etcetera). I was unable to set up authentication or a maximum total file size,
either.</p>

<p>Being fed up with ZoneMinder, I ditched it and decided to try and hack together
something of my own. After fiddling for the next half of the day with <code>ffmpeg</code>,
I came up with a viable solution, in bash.</p>

<p>``` bash bashsurv.sh <a href="https://gist.github.com/4100939">https://gist.github.com/4100939</a> View Gist</p>

<h1>!/bin/bash</h1>

<p>OUTPUT_DIR=&ldquo;/var/www/bashsurv&rdquo;
FFMPEG_INPUT_FLAGS=&ldquo;-rtsp_transport tcp&rdquo;
FFMPEG_SOURCE=&ldquo;rtsp://192.168.1.123/video.mp4&rdquo;
FFMPEG_OUTPUT_FLAGS=&ldquo;-r 20 -acodec libspeex&rdquo;
FFMPEG_OUTPUT_EXT=&ldquo;ogv&rdquo;
CLIP_LENGTH=600 # seconds
TIMELIMIT=620 # seconds, allows for network timeout over CLIP_LENGTH
KEEP_FILES_FOR=10080 # minutes</p>

<p>while [ true ]; do
  avconv -timelimit $TIMELIMIT $FFMPEG_INPUT_FLAGS -i $FFMPEG_SOURCE -t $CLIP_LENGTH $FFMPEG_OUTPUT_FLAGS $OUTPUT_DIR/$(date +%F.%T).$FFMPEG_OUTPUT_EXT
  if [ $? -ne 0 ] ;
  then</p>

<pre><code>sleep 1m ;
</code></pre>

<p>  fi
  find $OUTPUT_DIR/ -type f -mmin +$KEEP_FILES_FOR -delete
done
```</p>

<p>Unfortunately, I was not able to just copy the codec, which would have been
optimal in terms of resource (CPU/memory) usage on my server, but it doesn&rsquo;t
use up <em>that</em> much. Now I can set <code>OUTPUT_DIR</code> to somewhere in my <code>/var/www</code>,
set up authentication on it via <code>.htaccess</code> and be done with it. Dad can now
easily view the recordings using his browser, and if he really needs to view the
live stream I can just bookmark the RTSP stream in VLC for him or something.</p>

<p>Bottom line: K.I.S.S.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to fuck up with find]]></title>
    <link href="http://felix.oghina.com/2012/10/04/how-to-fuck-up-with-find/"/>
    <updated>2012-10-04T20:32:00+03:00</updated>
    <id>http://felix.oghina.com/2012/10/04/how-to-fuck-up-with-find</id>
    <content type="html"><![CDATA[<p>Welcome to the tutorial on how to successfully fuck up with <code>find</code>, the command line tool. In case you&rsquo;re not familiar with the <code>find</code> command, <a href="http://linux.die.net/man/1/find">RTFM</a>, it&rsquo;s freaking beautiful. However, in the hands of a non-RTFM-ing user (like me) it can be quite destructive. Let me demonstrate.</p>

<p>I wanted to use <code>find</code> to delete a bunch of files from a hierarchy, based on a simple name rule. So I thought I should use <a href="http://linux.die.net/man/1/xargs"><code>xargs</code></a> in combination with <code>find</code> to delete them. After a quick lookup of the <code>-print0</code> and <code>-name</code> arguments of the <code>find</code> command, I concluded that it would be a good idea to run the following command:</p>

<p><code>console
$ find . -print0 -name example | xargs -0 rm
</code></p>

<p>Can you guess what it did? Let me help you: it deleted all the files under <code>.</code>. The directories are still there, but they&rsquo;re not any help, are they? Now, fortunately for me, this was in a git repository that I had just pushed to a remote, so I was able to just clone it again.</p>

<p>In order to understand what happened, I had to actually read the man page carefully and pay attention. Apparently, everything that comes after the path (in my case <code>.</code>) on the command line is treated as an expression by <code>find</code>, where every argument evaluates to true or false, and is ORed with the next one. The <code>-print0</code> argument always returns true, because it&rsquo;s only meant to change the output, not the actual filtering. Because of this, my <code>-name</code> argument was completely ignored.</p>

<p>To conclude, the correct command would have been:</p>

<p><code>console
$ find . -name example -print0 | xargs -0 rm
</code></p>

<p>This way, the <code>-name</code> argument will take precedence over <code>print0</code>. However, because I actually spent time reading the manual this time, I discovered <code>find</code> also has a <code>-delete</code> argument, so <code>xargs</code> is not actually needed at all:</p>

<p><code>console
$ find . -name example -delete
</code></p>

<p>As I was saying, freaking beautiful.</p>
]]></content>
  </entry>
  
</feed>
